{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Model\n",
    "\n",
    "Looking at plots of the data in Excel, it looks like the classes have clear separation. Let's start with a Decision Tree Classifier model and see how that does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries we will be using\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn as skl\n",
    "import sklearn.metrics as skl_met\n",
    "import sklearn.model_selection as skl_modsel\n",
    "import sklearn.preprocessing as skl_pre\n",
    "import sklearn.tree as skl_tree\n",
    "import sklearn.pipeline as skl_pipeline\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version:     3.7.7 (default, Mar 23 2020, 23:19:08) [MSC v.1916 64 bit (AMD64)]\n",
      "numpy version:      1.18.1\n",
      "pandas version:     1.0.3\n",
      "sklearn version:    0.22.1\n"
     ]
    }
   ],
   "source": [
    "# Always a good idea to dump verions of key libraries\n",
    "print('python version:    ', sys.version)\n",
    "print('numpy version:     ', np.__version__)\n",
    "print('pandas version:    ', pd.__version__)\n",
    "print('sklearn version:   ', skl.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I read it is a good idea to seed the Numpy random number generator\n",
    "#   so that we get the same 'random' numbers each run for reproducibility. \n",
    "np.random.seed(123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loaad the dataset\n",
    "dataset  = pd.read_csv('./ML_20.csv', header=None, names=['status', 'vibration'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 5994\n",
      "\n",
      "   status  vibration\n",
      "0       0   0.078453\n",
      "1       0   0.078453\n",
      "2       0   0.098067\n",
      "3       0   0.094144\n",
      "4       0   0.084991 \n",
      "\n",
      "      status  vibration\n",
      "1000       2   0.575323\n",
      "1001       2   0.578592\n",
      "1002       2   0.611935\n",
      "1003       2   0.594937\n",
      "1004       2   0.537965 \n",
      "\n",
      "      status  vibration\n",
      "2000       1   0.264780\n",
      "2001       1   0.274586\n",
      "2002       1   0.268048\n",
      "2003       1   0.263379\n",
      "2004       1   0.250070 \n",
      "\n",
      "Class   Count   % of Total\n",
      "  0       999     33.33%\n",
      "  1       999     33.33%\n",
      "  2       999     33.33%\n"
     ]
    }
   ],
   "source": [
    "# Dump some general stats\n",
    "print('Number of records: {}\\n'.format(dataset.size))\n",
    "\n",
    "# Dump a few records from each 'section'; there should be 999 record with status=0,\n",
    "#  followed by 999 records with status=2, and 999 records with status=1\n",
    "print(dataset[0:5], '\\n')\n",
    "print(dataset[1000:1005], '\\n')\n",
    "print(dataset[2000:2005], '\\n')\n",
    "\n",
    "# For a categorical values, print count and % of each class\n",
    "counts = dataset['status'].value_counts().sort_index()\n",
    "print('Class   Count   % of Total')\n",
    "for idx, val in counts.items():\n",
    "    print('  {0}    {1:6d}    {2:6.2f}%'.format(idx, val, ((val / dataset.shape[0]) * 100)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split off the the features and target (note we only have 1 feature)\n",
    "# Since we constructed the data to have status values 0-2, we can just\n",
    "#  use those values as the class labels without any further fuddling (e.g. label encoding)\n",
    "X = dataset.loc[:, 'vibration']\n",
    "y_class = dataset.loc[:, 'status']\n",
    "\n",
    "# Since we only have 1 feature, the above will generate a 1-dim array; we need\n",
    "#  a 2-dim array with only 1 column; so need to add an extra dimension, the columns\n",
    "X = np.expand_dims(X, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape             = (2997, 1)\n",
      "X_train.shape       = (2397, 1)\n",
      "X_test.shape        = (600, 1)\n",
      "y_train_class.shape = (2397,)\n",
      "y_test_class.shape  = (600,)\n",
      "\n",
      "\n",
      "Target class counts - train:\n",
      "    Class: 0  Count: 799\n",
      "    Class: 1  Count: 799\n",
      "    Class: 2  Count: 799\n",
      "\n",
      "\n",
      "Target class distribution - test:\n",
      "    Class: 0  Count: 200\n",
      "    Class: 1  Count: 200\n",
      "    Class: 2  Count: 200\n"
     ]
    }
   ],
   "source": [
    "# Split into training and test\n",
    "X_train, X_test, y_train_class, y_test_class = skl_modsel.train_test_split(X, y_class, test_size=0.2, \n",
    "                                                                           random_state=0, stratify=y_class)\n",
    "\n",
    "print(\"X.shape             =\", X.shape)\n",
    "print(\"X_train.shape       =\", X_train.shape)\n",
    "print(\"X_test.shape        =\", X_test.shape)\n",
    "print(\"y_train_class.shape =\", y_train_class.shape)\n",
    "print(\"y_test_class.shape  =\", y_test_class.shape)\n",
    "\n",
    "print('\\n')\n",
    "print('Target class counts - train:')\n",
    "cname, count = np.unique(y_train_class, return_counts=True)\n",
    "for ele in zip(cname, count):\n",
    "    print('    Class: {}  Count: {}'.format(ele[0], ele[1]))\n",
    "\n",
    "print('\\n')\n",
    "print('Target class distribution - test:')\n",
    "cname, count = np.unique(y_test_class, return_counts=True)\n",
    "for ele in zip(cname, count):\n",
    "    print('    Class: {}  Count: {}'.format(ele[0], ele[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use the standard scaler to normalize feature values\n",
    "sc_X = skl_pre.StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# At long last, we can create a model\n",
    "tree_model = skl_tree.DecisionTreeClassifier()    # kernel='linear')\n",
    "tree_model.fit(X_train, y_train_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix - Test:\n",
      " [[200   0   0]\n",
      " [  0 200   0]\n",
      " [  0   0 200]]\n",
      "\n",
      "\n",
      "Precision - Test: [1. 1. 1.]\n",
      "Recall - Test   : [1. 1. 1.]\n",
      "F1 - Test       : [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Using the test set, let's see how well our model does\n",
    "yhat_test_class = tree_model.predict(X_test)\n",
    "\n",
    "print('Confusion Matrix - Test:\\n', skl_met.confusion_matrix(y_test_class, yhat_test_class))\n",
    "print('\\n')\n",
    "print('Precision - Test:', skl_met.precision_score(y_test_class, yhat_test_class, average=None))\n",
    "print('Recall - Test   :', skl_met.recall_score(y_test_class, yhat_test_class, average=None))\n",
    "print('F1 - Test       :', skl_met.f1_score(y_test_class, yhat_test_class, average=None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix - Train:\n",
      " [[799   0   0]\n",
      " [  0 799   0]\n",
      " [  0   0 799]]\n",
      "\n",
      "\n",
      "Precision - Train: [1. 1. 1.]\n",
      "Recall - Train   : [1. 1. 1.]\n",
      "F1 - Train       : [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Let's also see how the model did on the training set\n",
    "yhat_train_class = tree_model.predict(X_train)\n",
    "\n",
    "print('Confusion Matrix - Train:\\n', skl_met.confusion_matrix(y_train_class, yhat_train_class))\n",
    "print('\\n')\n",
    "print('Precision - Train:', skl_met.precision_score(y_train_class, yhat_train_class, average=None))\n",
    "print('Recall - Train   :', skl_met.recall_score(y_train_class, yhat_train_class, average=None))\n",
    "print('F1 - Train       :', skl_met.f1_score(y_train_class, yhat_train_class, average=None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's package the data standardization and decision tree into a pipeline\n",
    "pipeline = skl_pipeline.make_pipeline(sc_X, tree_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix - Pipeline Test:\n",
      " [[2 0 0]\n",
      " [0 2 0]\n",
      " [0 0 2]]\n"
     ]
    }
   ],
   "source": [
    "# Let's test the pipeline with some randomly selected values from the \n",
    "#   original data file; this should predict classes 0, 2, 1\n",
    "X_pipeline_test = [[0.074531], [0.847295], [0.194172], [0.203978], [0.074531], [0.786493]]\n",
    "y_pipeline_test_class = [0, 2, 1, 1, 0, 2]\n",
    "\n",
    "yhat_pipeline_test_class = pipeline.predict(X_pipeline_test)\n",
    "\n",
    "print('Confusion Matrix - Pipeline Test:\\n', skl_met.confusion_matrix(y_pipeline_test_class, yhat_pipeline_test_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model off\n",
    "pickle.dump(pipeline, open('tree_pipeline.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
